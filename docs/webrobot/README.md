# 9장 웹 로봇

## 웹 크롤러란

웹 페이지와 그 페이지가 가리키는 모든 웹 페이지를 재귀적으로 순회하며 가져오는 로봇이다. HTML 하이퍼링크들로 만들어진 웹을 따라 기어다니기(crawl)떄문에 크롤러라 부른다.

일반적으로 좋은 루트 집합은 크고 인기 있는 웹사이트, 새로 생성된 페이지들의 목록, 그리고 자주 링크되지 않는 잘 알려져 있지 않은 페이지들의 목록으로 구성되어 있다. 
크롤러는 웹을 돌아다니면서 검색한 각 페이지 안에 들어있는 URL 링크들을 파싱해서 크롤링할 페이지들의 목록에 추가한다.

## 자료구조

수억개의 URL은 빠른 검색 구조를 요구하기 때문에 빠른 속도는 중요하다. URL 목록의 완벽한 검색은 불가능하지만, 적어도 검색트리나 해시테이블을 필요로 할 것이다. 
가령, URL이 평균 40바이트라 할 때, 5억개의 URL을 크롤링했다면 검색 데이터는 20GB 이상의 메모리를 요구할 것이다.

## 루프와 중복

1. 루프에 빠져 크롤러가 네트워크 대역폭을 다 차지하여 일반 사용자도 사이트에 접근할 수 없을 수 있다.
2. 크롤러 자신도 중복 컨텐츠를 가지게 됨으로써 똑같은 페이지를 반환하는 인터넷 검색엔진이 된다.

- 로봇들은 순환을 피하기 위해 반드시 어디를 방문했는지 알아야 한다.


## 순환 피하기 


- URL 정규화

포트, 태그, 대소문자, 이스케이핑 문자, index.html, IP 등 표현은 다르지만 같은 리소스를 가리키는 경우가 있다. 
URL을 표준 형태로 변환함으로써, 같은 리소스를 가리키는 중복된 URL이 생기는 것을 일부 회피한다.

1. 포트번호가 명시되지 않았다면 호스트명에 ':80'을 추가한다.
2. 모든 %xx 이스케이핑된 문자들을 대응하는 문자로 변환한다.
3. \#태그들을 제거한다.

- 너비 우선 크롤링 : 방문할 URL들을 웹사이트 전체에 걸쳐 너비 우선으로 스케줄링한다면 로봇함정을 건드리게 되더라도 깊이 우선방식처럼 영원히 다른 사이트로 빠져나오게 되는 상황은 피할 수 있다.

- 스로틀링 : 만약 로봇이 순환을 건드려서 지속적으로 그 사이트의 별칭들에 접근을 시도한다면, 스로틀링을 이용해 그 서버에 대한 접근 횟수와 중복의 총 횟수를 제한할 수 있다.

- URL 크기 제한 : 일정 길이를 넘는 URL 크롤링을 거부할 수 있다. 다만이 경우 가져오지 못하는 컨텐츠들도 있을 것이다.

- 블랙리스트 작성

- 패턴 발견 : 루프에 따른 일정 패턴을 탐지하고 URL을 크롤링하는 것을 거절할 수 있다.

심볼릭 링크가 잘못 구성되어 있을 경우에도 로봇은 루프에 빠질 수 있다. 이 경우 URL의 길이가 로봇이나 서버의 한계를 넘을 때까지 이 순환은 계속된다. 
1. GET http://www.foo.com/index.html
2. GET http://www.foo.com/subdir/index.html
3. GET http://www.foo.com/subdir/subdir/index.html

- fingerprint: 페이지의 컨텐츠에서  몇 바이트를 얻어내어 MD5등을 이용하여 checksum을 계산한다. 이전에 보았던 checksum이라면 그 페이지의 링크는 크롤링하지 않는다. 

## 로봇의 요청헤더 식별하기

1. User-Agent: 서버에게 요청을 만든 로봇의 이름을 알려준다.
2. From: 로봇의 사용자/관리자의 이메일 주소를 제공한다.
3. Accept: 서버에게 어떤미디어 타입을 보내도 되는지 말해준다. 
4. Referer: 현재의 URL을 포함한 문서의 URL을 제공한다.

- 가상 호스팅을 하는 서버들이 있으므로 Host 헤더를 포함하지 않은 크롤러가 두 개의 사이트를 운영하는 서버에 요청할 경우, 예상과 다른 결과를 수집할 수 있다.
- 로봇이 검색하는 컨텐츠의 양을 최소화하기 위해 오직 변경되었을 때만 컨텐츠를 가져오도록 하는 것도 의미가 있다.
- 사이트 관리자는 풍부한 기능을 갖추지 못한 브라우저나 로봇 등 다양한 클라이언트에 잘 대응하는 유연한 페이지를 개발하여야 한다.


## 로봇 차단하기

가상 docroot에 있는 robots.txt 파일에는 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보가 담겨있다. 
로봇이 robots.txt를 서버에 요청한 후 응답코드에 따라 다르게 동작한다.

- 응답코드에 따른 동작
2XX: 응답 컨텐츠를 파싱하여 차단 규칙을 얻고 그 사이트에 요청할 때 그 규칙에 따른다.
404: 로봇은 활성화된 차단 규칙이 존재하지 않는다고 가정하고 robots.txt의 제약없이 그 사이트에 접근할 수 있다.
401 혹은 403: 로봇은 그 사이트로의 접근은 완전히 제한되어 있다고 가정해야 한다.
503: 로봇은 그 사이트의 리로스를 검색하는 것을 뒤로 미룬다.
3XX: 로봇은 리소스가 발견될때까지 리다이렉트를 따라가야 한다.

robots.txt파일은 User-Agent와 Allow/Disallow 라인으로 구성된다. 
